
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Evaluating the Potential of Large Language Models for Generating Scientific Feedback on Research Papers</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
        }
        .toc {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 20px;
        }
        .toc ul {
            list-style-type: none;
            padding-left: 20px;
        }
        .toc ul ul {
            padding-left: 20px;
        }
        .section {
            margin-bottom: 30px;
            border-bottom: 1px solid #eee;
            padding-bottom: 20px;
        }
        .quote {
            background-color: #e7f4ff;
            border-left: 5px solid #3498db;
            padding: 10px;
            margin: 10px 0;
            font-style: italic;
        }
        .back-to-top {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background-color: #3498db;
            color: white;
            padding: 10px 15px;
            border-radius: 5px;
            text-decoration: none;
            display: none;
        }
        details {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 4px;
            margin-bottom: 10px;
        }
        summary {
            padding: 10px;
            cursor: pointer;
            font-weight: bold;
            background-color: #f0f0f0;
        }
        details > * {
            padding: 10px;
        }
        .critique-section {
            margin-bottom: 15px;
            padding: 10px;
            background-color: #f9f9f9;
            border-radius: 4px;
        }
        .critique-title {
            font-weight: bold;
            margin-bottom: 10px;
            color: #2c3e50;
        }
        .aspect {
            margin-bottom: 10px;
            padding: 10px;
            background-color: #ffffff;
            border-radius: 4px;
        }
        .strength {
            border-left: 3px solid #2ecc71;
        }
        .suggestion {
            border-left: 3px solid #e74c3c;
        }
        .aspect-type {
            font-weight: bold;
            margin-bottom: 5px;
        }
        .non-text-description {
            padding-left: 20px;
            margin-left: 10px;
        }
        .non-text-element {
            margin-bottom: 15px;
            padding: 10px;
            background-color: #f9f9f9;
            border-radius: 4px;
        }
        .non-text-element ul {
            padding-left: 30px;
        }
        .first-mention {
            margin-top: 10px;
            padding: 10px;
            background-color: #f0f8ff;
            border-radius: 4px;
        }
        .first-mention h5 {
            margin-top: 0;
            color: #2c3e50;
        }
        @media (max-width: 600px) {
            body {
                padding: 10px;
            }
        }
    </style>
</head>
<body>
    <h1>Evaluating the Potential of Large Language Models for Generating Scientific Feedback on Research Papers</h1>
    
    <div class="toc">
        <h2>Table of Contents</h2>
        <ul>
            <li><a href="#overall-summary">Overall Summary</a></li>
            <li><a href="#section-analysis">Section Analysis</a>
                <ul>
                <li><a href="#section-0">Abstract</a></li><li><a href="#section-1">Introduction</a></li><li><a href="#section-2">Results</a></li><li><a href="#section-3">Discussion</a></li><li><a href="#section-4">Methods</a></li>
                </ul>
            </li>
        </ul>
    </div>
    
    <div id="overall-summary" class="section">
        <h2>Overall Summary</h2>
        <h3>Overview</h3>
        <p>This study investigates the feasibility of using large language models (LLMs), specifically GPT-4, to provide feedback on scientific papers. The authors developed an automated pipeline that uses GPT-4 to generate feedback and evaluated its quality through a retrospective analysis comparing it to human peer reviews and a prospective user study assessing researcher perceptions.</p>
        
        <h3>Key Findings</h3>
        <ul>
        <li>GPT-4 generated feedback overlapped significantly with human peer reviews, with over 57% of GPT-4&#x27;s comments also raised by human reviewers in a dataset of Nature family journals and over 77% in a dataset from the ICLR machine learning conference.</li><li>The overlap between GPT-4&#x27;s feedback and human reviews was comparable to the inter-reviewer agreement, suggesting that LLM feedback can be as consistent as human feedback.</li><li>GPT-4&#x27;s feedback was found to be paper-specific, as shuffling the feedback across different papers significantly reduced the overlap with human reviews.</li><li>LLM feedback tended to emphasize certain aspects of scientific evaluation more than human reviewers, such as the implications of research, while commenting less on novelty.</li><li>Over half of the researchers in a user study (57.4%) found GPT-4 generated feedback helpful or very helpful, and 82.4% found it more beneficial than feedback from at least some human reviewers.</li>
        </ul>
        
        <h3>Strengths</h3>
        <ul>
        <li>The study employed a robust methodology involving both a retrospective analysis comparing LLM feedback to existing human reviews and a prospective user study assessing researcher perceptions.</li><li>The use of two large and diverse datasets (Nature family journals and ICLR) strengthens the generalizability of the findings.</li><li>The study used rigorous quantitative analysis to assess the overlap between LLM and human feedback, providing objective evidence for the LLM&#x27;s capabilities.</li><li>The human verification process used to validate the comment matching pipeline ensures the reliability of the overlap analysis.</li><li>The study provides a balanced perspective on the potential and limitations of LLMs for scientific feedback, acknowledging both the promising findings and the areas where LLMs fall short.</li>
        </ul>
        
        <h3>Areas for Improvement</h3>
        <ul>
        <li>A qualitative analysis of the content of LLM feedback would complement the quantitative findings and provide insights into the strengths and weaknesses of the feedback in terms of its content, specificity, and actionability.</li><li>Comparing GPT-4&#x27;s performance with other LLMs, such as those specifically trained on scientific literature, would provide a broader perspective on the potential of LLMs for scientific feedback.</li><li>A deeper discussion of the ethical implications of using LLMs for scientific feedback, including potential biases and the impact on the integrity of the peer review process, would strengthen the analysis.</li>
        </ul>
        
        <h3>Significant Elements</h3>
        
    <div>
        <h4>Figure 2</h4>
        <p><strong>Description:</strong> Figure 2 presents a comprehensive retrospective analysis of LLM and human scientific feedback, comparing the overlap between comments raised by GPT-4 and human reviewers across different journals and decision outcomes.</p>
        <p><strong>Relevance:</strong> This figure provides strong evidence for the study&#x27;s main claim that LLM-generated feedback aligns with human feedback, demonstrating a substantial overlap comparable to inter-reviewer agreement.</p>
    </div>
    
    <div>
        <h4>Figure 4</h4>
        <p><strong>Description:</strong> Figure 4 presents the results of the prospective user study, illustrating researcher perceptions of the helpfulness, alignment, specificity, and potential benefits of LLM feedback.</p>
        <p><strong>Relevance:</strong> This figure provides valuable insights into user acceptance and potential adoption of LLM-based feedback systems, complementing the retrospective analysis with subjective evaluations.</p>
    </div>
    
        
        <h3>Conclusion</h3>
        <p>This study demonstrates the potential of LLMs, specifically GPT-4, to provide valuable feedback on scientific papers. While LLM feedback cannot replace expert human review, it can be a useful tool for researchers, particularly those with limited access to traditional feedback mechanisms. Future research should explore the use of LLMs for error detection and correction, expand language coverage, and address the ethical implications of using LLMs in scientific evaluation.</p>
    </div>
    
    <div id="section-analysis" class="section">
        <h2>Section Analysis</h2>
        
        <div id="section-0" class="section">
            <h3>Abstract</h3>
            
            <h4>Overview</h4>
            <p>This abstract presents a study investigating the potential of large language models (LLMs), specifically GPT-4, to provide useful feedback on research papers. The authors developed an automated pipeline using GPT-4 to generate feedback on scientific papers and evaluated its quality through two large-scale studies, comparing it to human peer reviewer feedback and gathering user perceptions from researchers.</p>
            
            <h4>Key Aspects</h4>
            <ul><li><strong>LLM-Generated Feedback:</strong>The study used GPT-4 to generate feedback on full PDFs of scientific papers, focusing on aspects like significance, novelty, acceptance/rejection reasons, and improvement suggestions.</li><li><strong>Retrospective Analysis:</strong>GPT-4&#x27;s feedback was compared to human peer reviews from 15 Nature journals (3,096 papers) and the ICLR machine learning conference (1,709 papers). The overlap in points raised was comparable between GPT-4 and human reviewers, and even higher for weaker papers.</li><li><strong>Prospective User Study:</strong>A survey of 308 researchers from 110 US institutions in AI and computational biology assessed their perceptions of GPT-4 feedback on their own papers. Over half found it helpful, and many found it more beneficial than some human reviews.</li><li><strong>Limitations:</strong>The study acknowledges limitations, such as GPT-4&#x27;s tendency to focus on certain feedback aspects and struggle with in-depth critique of method design.</li><li><strong>Complementary Role:</strong>The authors suggest that LLM and human feedback can complement each other, with human review remaining the foundation of the scientific process while LLMs offer benefits, especially when expert feedback is limited.</li></ul>
            
            <h4>Strengths</h4>
            <ul>
            
    <li>
        <strong>Clear Research Question</strong>
        <p>The abstract effectively establishes the central research question: Can LLMs provide useful feedback on research papers? This question is directly addressed through the study&#x27;s design and findings.</p>
        <div class="quote">"With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied." (Page 1)</div>
    </li>
    
    <li>
        <strong>Strong Methodology</strong>
        <p>The abstract outlines a robust methodology involving both retrospective analysis (comparing GPT-4 feedback to existing human reviews) and a prospective user study. This two-pronged approach provides a comprehensive evaluation of the LLM&#x27;s capabilities.</p>
        <div class="quote">"We evaluated the quality of GPT-4’s feedback through two large-scale studies. We first quantitatively compared GPT-4’s generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers." (Page 1)</div>
    </li>
    
    <li>
        <strong>Concise Presentation of Findings</strong>
        <p>The abstract effectively summarizes the key findings of both the retrospective analysis and the user study, highlighting the overlap between LLM and human feedback, the perceived helpfulness of LLM feedback, and the limitations identified.</p>
        <div class="quote">"Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations. For example, GPT-4 tends to focus on certain aspects of scientific feedback (e.g., ‘add experiments on more datasets’), and often struggles to provide in-depth critique of method design." (Page 1)</div>
    </li>
    
            </ul>
            
            <h4>Suggestions for Improvement</h4>
            <ul>
            
    <li>
        <strong>Expand on User Study Demographics</strong>
        <p>While the abstract mentions the number of researchers in the user study, it would be beneficial to briefly mention key demographics (e.g., career stage, research area) to provide a better understanding of the study&#x27;s generalizability.</p>
        <div class="quote">"We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers." (Page 1)</div>
        <p><strong>Rationale:</strong> Providing a glimpse into the user demographics would strengthen the abstract by demonstrating the diversity of the study sample and its potential relevance to a broader audience.</p>
        <p><strong>Implementation:</strong> Add a brief phrase mentioning key demographics, such as &quot;...representing diverse career stages and research areas within AI and computational biology...&quot;</p>
    </li>
    
    <li>
        <strong>Elaborate on Specific Limitations</strong>
        <p>The abstract mentions that GPT-4 struggles with in-depth critique of method design. Briefly elaborating on this limitation with a specific example would enhance the abstract&#x27;s impact.</p>
        <div class="quote">"For example, GPT-4 tends to focus on certain aspects of scientific feedback (e.g., ‘add experiments on more datasets’), and often struggles to provide in-depth critique of method design." (Page 1)</div>
        <p><strong>Rationale:</strong> Providing a concrete example of this limitation would make the abstract more informative and highlight areas for future research.</p>
        <p><strong>Implementation:</strong> Add a concise example, such as &quot;...for instance, GPT-4 was less likely to identify flaws in statistical analysis methods compared to human reviewers...&quot;</p>
    </li>
    
    <li>
        <strong>Strengthen Concluding Sentence</strong>
        <p>The final sentence, while stating the potential benefits of LLMs, could be strengthened by explicitly mentioning the need for further research and development to address the identified limitations.</p>
        <div class="quote">"While human expert review is and should continue to be the foundation of rigorous scientific process, LLM feedback could benefit researchers, especially when timely expert feedback is not available and in earlier stages of manuscript preparation before peer-review." (Page 1)</div>
        <p><strong>Rationale:</strong> A stronger concluding sentence would leave a lasting impression on the reader and emphasize the ongoing nature of this research area.</p>
        <p><strong>Implementation:</strong> Revise the sentence to include a call for future research, such as &quot;...further research is needed to address these limitations and fully realize the potential of LLMs as valuable tools for researchers...&quot;</p>
    </li>
    
            </ul>
            
            
        </div>
        
        <div id="section-1" class="section">
            <h3>Introduction</h3>
            
            <h4>Overview</h4>
            <p>The introduction section emphasizes the crucial role of feedback in scientific progress, highlighting the challenges posed by the increasing volume of research and knowledge specialization. It argues for the need for scalable feedback mechanisms and introduces the potential of large language models (LLMs) like GPT-4 to address this need. The section concludes by outlining the study&#x27;s objectives and methodology, focusing on a GPT-4 based feedback generation pipeline and its evaluation through retrospective analysis and a prospective user study.</p>
            
            <h4>Key Aspects</h4>
            <ul><li><strong>Importance of Feedback:</strong>The section begins by underscoring the significance of feedback in scientific research, emphasizing its role in promoting discoveries, shaping paradigms, and fostering constructive dialogue among scientists.</li><li><strong>Challenges in Feedback:</strong>The authors discuss the challenges associated with providing timely and insightful feedback, particularly the increasing volume of scholarly publications and the deepening specialization of scientific knowledge. They highlight the limitations of traditional feedback avenues like peer review, including scalability, expertise accessibility, and promptness.</li><li><strong>Scientific Inequality:</strong>The section points out the unequal access to feedback, particularly for researchers from marginalized backgrounds or resource-limited settings, further exacerbating existing scientific inequalities.</li><li><strong>Potential of LLMs:</strong>The introduction introduces large language models (LLMs) as a potential solution to these challenges, suggesting their ability to provide scalable and efficient feedback mechanisms. It acknowledges the growing interest in using LLMs for scientific feedback but notes the lack of systematic studies on their utility.</li><li><strong>Study Objectives and Methodology:</strong>The section outlines the study&#x27;s goals, which are to systematically analyze the reliability and credibility of LLMs for generating scientific feedback. It describes the development of a GPT-4 based feedback generation pipeline and the two-pronged evaluation approach: a retrospective analysis comparing LLM feedback to human peer reviews and a prospective user study assessing researcher perceptions of LLM feedback.</li></ul>
            
            <h4>Strengths</h4>
            <ul>
            
    <li>
        <strong>Compelling Motivation</strong>
        <p>The introduction effectively establishes the need for alternative feedback mechanisms by highlighting the challenges and limitations of traditional approaches. The discussion of scientific inequality adds further weight to the argument.</p>
        <div class="quote">"While shortage of high-quality feedback presents a fundamental constraint on the sustainable growth of science overall, it also becomes a source of deepening scientific inequalities. Marginalized researchers, especially those from non-elite institutions or resource-limited regions, often face disproportionate challenges in accessing valuable feedback, perpetuating a cycle of systemic scientific inequality13, 14." (Page 2)</div>
    </li>
    
    <li>
        <strong>Clear Research Gap</strong>
        <p>The authors clearly identify the research gap by acknowledging the growing interest in LLMs for scientific feedback but pointing out the lack of systematic studies on their effectiveness and reliability.</p>
        <div class="quote">"While LLMs have made remarkable strides in various domains, the promises and perils of leveraging LLMs for scientific feedback remain largely unknown. Despite recent attempts that explore the potential uses of such tools in areas such as automating paper screening24, error identification25, and checklist verification26 1, we lack large-scale empirical evidence on whether and how LLMs may be used to facilitate scientific feedback and augment current academic practices." (Page 2)</div>
    </li>
    
    <li>
        <strong>Well-Defined Methodology</strong>
        <p>The introduction provides a clear overview of the study&#x27;s methodology, including the development of the GPT-4 pipeline and the two-pronged evaluation approach. This transparency strengthens the reader&#x27;s confidence in the study&#x27;s rigor.</p>
        <div class="quote">"In this work, we present the first large-scale systematic analysis characterizing the potential reliability and credibility of leveraging LLM for generating scientific feedback. Specifically, we developed a GPT-4 based scientific feedback generation pipeline that takes the raw PDF of a paper and produces structured feedback (Fig. 1a). The system is designed to generate constructive feedback across various key aspects, mirroring the review structure of leading interdisciplinary journals27, 28 and conferences29–33, including: 1) Significance and novelty, 2) Potential reasons for acceptance, 3) Potential reasons for rejection, and 4) Suggestions for improvement." (Page 2)</div>
    </li>
    
            </ul>
            
            <h4>Suggestions for Improvement</h4>
            <ul>
            
    <li>
        <strong>Expand on Ethical Considerations</strong>
        <p>While the introduction briefly mentions the potential misuse of LLMs, a more detailed discussion of the ethical implications of using LLMs for scientific feedback would strengthen the section.</p>
        <div class="quote">"While LLMs have made remarkable strides in various domains, the promises and perils of leveraging LLMs for scientific feedback remain largely unknown." (Page 2)</div>
        <p><strong>Rationale:</strong> A deeper exploration of ethical considerations would demonstrate the authors&#x27; awareness of the potential risks and biases associated with LLMs, enhancing the study&#x27;s credibility and fostering responsible AI development.</p>
        <p><strong>Implementation:</strong> Add a paragraph discussing potential ethical concerns, such as bias in LLM-generated feedback, the potential for misuse by authors to artificially inflate their work&#x27;s quality, and the impact on the integrity of the peer review process. This could include references to existing literature on AI ethics in research.</p>
    </li>
    
    <li>
        <strong>Clarify Scope of &quot;Scientific Feedback&quot;</strong>
        <p>The introduction refers to &quot;scientific feedback&quot; without explicitly defining its scope. A more precise definition would enhance the clarity and focus of the study.</p>
        <div class="quote">"In this work, we present the first large-scale systematic analysis characterizing the potential reliability and credibility of leveraging LLM for generating scientific feedback." (Page 2)</div>
        <p><strong>Rationale:</strong> A clear definition of &quot;scientific feedback&quot; would help readers understand the specific aspects of feedback the study is addressing, avoiding potential misinterpretations and setting clear expectations for the evaluation.</p>
        <p><strong>Implementation:</strong> Include a sentence or two defining &quot;scientific feedback&quot; in the context of the study. For example, &quot;In this study, we define scientific feedback as constructive criticism and suggestions aimed at improving the quality, rigor, and clarity of research manuscripts, encompassing aspects such as significance, novelty, methodology, and presentation.&quot;</p>
    </li>
    
    <li>
        <strong>Connect to Broader Scientific Landscape</strong>
        <p>The introduction focuses on the immediate challenges of scientific feedback but could benefit from connecting this issue to broader trends in scientific publishing and knowledge production.</p>
        <div class="quote">"Given these challenges, there is an urgent need for crafting scalable and efficient feedback mechanisms that can enrich and streamline the scientific feedback process." (Page 2)</div>
        <p><strong>Rationale:</strong> Linking the study to broader scientific trends would provide a richer context for the research and highlight its potential impact beyond the specific issue of feedback.</p>
        <p><strong>Implementation:</strong> Add a sentence or two discussing how the increasing volume of research, the rise of interdisciplinary fields, and the growing importance of open science practices contribute to the need for innovative feedback mechanisms. This could include references to relevant literature on the changing landscape of scientific publishing.</p>
    </li>
    
            </ul>
            
            
        </div>
        
        <div id="section-2" class="section">
            <h3>Results</h3>
            
            <h4>Overview</h4>
            <p>The Results section details the development and evaluation of an automated pipeline using GPT-4 to generate feedback on scientific papers. It presents a retrospective analysis comparing GPT-4&#x27;s feedback with human peer reviews from Nature family journals and the ICLR machine learning conference, demonstrating a significant overlap in the points raised. The section also highlights key findings, including the observation that LLM feedback is paper-specific and tends to align with human reviewers on major comments, but also emphasizes certain aspects more than humans.</p>
            
            <h4>Key Aspects</h4>
            <ul><li><strong>LLM Feedback Generation Pipeline:</strong>The authors developed an automated pipeline that uses GPT-4 to generate feedback on scientific papers, taking the raw PDF as input and producing structured feedback covering significance, novelty, acceptance/rejection reasons, and improvement suggestions.</li><li><strong>Retrospective Evaluation:</strong>The pipeline&#x27;s effectiveness was evaluated by comparing GPT-4&#x27;s feedback with human peer reviews from two datasets: 3,096 papers from Nature family journals and 1,709 papers from the ICLR machine learning conference.</li><li><strong>Overlap Analysis:</strong>The study found a substantial overlap between GPT-4&#x27;s feedback and human reviews, with over 57% of GPT-4&#x27;s comments also raised by human reviewers in the Nature dataset and over 77% in the ICLR dataset. The overlap was comparable to the inter-reviewer agreement.</li><li><strong>Specificity of LLM Feedback:</strong>A shuffling experiment demonstrated that GPT-4&#x27;s feedback is paper-specific and not just generic comments applicable to any paper. Shuffling LLM feedback across papers significantly reduced the overlap with human reviews.</li><li><strong>Emphasis on Specific Aspects:</strong>Analysis of the ICLR dataset revealed that GPT-4 tends to emphasize certain aspects of feedback more than human reviewers, such as the implications of research, while commenting less on novelty.</li></ul>
            
            <h4>Strengths</h4>
            <ul>
            
    <li>
        <strong>Comprehensive Methodology</strong>
        <p>The section presents a well-structured and comprehensive methodology for evaluating the LLM feedback generation pipeline. The use of two large and diverse datasets (Nature and ICLR) strengthens the generalizability of the findings.</p>
        <div class="quote">"To characterize the informativeness of GPT-4 generated feedback, we conducted both a retrospective analysis and a prospective user study. In the retrospective analysis, we applied our pipeline on papers that had previously been assessed by human reviewers. We then compared the LLM feedback with the human feedback." (Page 3)</div>
    </li>
    
    <li>
        <strong>Quantitative Analysis</strong>
        <p>The section employs rigorous quantitative analysis to assess the overlap between LLM and human feedback, using metrics like hit rate, Szymkiewicz–Simpson overlap coefficient, Jaccard index, and Sørensen–Dice coefficient. This quantitative approach provides objective evidence for the pipeline&#x27;s effectiveness.</p>
        <div class="quote">"We developed a retrospective comment matching pipeline to evaluate the overlap between feedback from LLM and human reviewers (Fig. 1b, Methods). The pipeline first performs extractive text summarization34–37 to extract the comments from both LLM and human-written feedback. It then applies semantic text matching38–40 to identify shared comments between the two feedback sources." (Page 3)</div>
    </li>
    
    <li>
        <strong>Detailed Findings</strong>
        <p>The section provides a detailed presentation of the findings, including specific overlap percentages, trends across different journals and decision outcomes, and insights into the characteristics of LLM comments. This level of detail enhances the transparency and informativeness of the research.</p>
        <div class="quote">"More than half (57.55%) of the comments raised by GPT-4 were raised by at least one human reviewer (Supp. Fig. 1a). This suggests a considerable overlap between LLM feedback and human feedback, indicating potential accuracy and usefulness of the system. When comparing LLM feedback with comments from each individual reviewer, approximately one third (30.85%) of GPT-4 raised comments overlapped with comments from an individual reviewer (Fig. 2a)." (Page 3)</div>
    </li>
    
            </ul>
            
            <h4>Suggestions for Improvement</h4>
            <ul>
            
    <li>
        <strong>Qualitative Analysis of Feedback Content</strong>
        <p>While the section quantifies the overlap between LLM and human feedback, it would benefit from a qualitative analysis of the content of the feedback. Examining the nature of the comments, their level of detail, and their potential usefulness to authors would provide a richer understanding of the LLM&#x27;s capabilities.</p>
        <div class="quote">"We assessed the degree of overlap between key points raised by both sources to gauge the effectiveness and reliability of LLM feedback." (Page 2)</div>
        <p><strong>Rationale:</strong> A qualitative analysis would complement the quantitative findings and provide insights into the strengths and weaknesses of LLM feedback in terms of its content, specificity, and actionability.</p>
        <p><strong>Implementation:</strong> Include a subsection or paragraph discussing the qualitative aspects of LLM feedback. This could involve manually analyzing a sample of comments from both LLM and human reviewers, categorizing them based on their content (e.g., suggestions for improvement, critiques of methodology, identification of errors), and assessing their level of detail and potential usefulness to authors.</p>
    </li>
    
    <li>
        <strong>Comparison with Other LLMs</strong>
        <p>The study focuses solely on GPT-4. Comparing its performance with other LLMs, such as those specifically trained on scientific literature, would provide a broader perspective on the potential of LLMs for scientific feedback.</p>
        <div class="quote">"We developed an automated pipeline that utilizes OpenAI’s GPT-419 to generate feedback on the full PDF of scientific papers." (Page 3)</div>
        <p><strong>Rationale:</strong> Comparing GPT-4 with other LLMs would help assess its relative strengths and weaknesses and identify potential areas for improvement. It would also contribute to a more comprehensive understanding of the capabilities of different LLMs for scientific feedback.</p>
        <p><strong>Implementation:</strong> Extend the study to include other LLMs, such as SciBERT or BioBERT, which are specifically trained on scientific text. Compare their performance with GPT-4 in terms of overlap with human reviews, specificity of feedback, and emphasis on different aspects of scientific writing.</p>
    </li>
    
    <li>
        <strong>Analysis of Feedback on Rejected Papers</strong>
        <p>The section primarily focuses on accepted papers from Nature journals. Analyzing LLM feedback on rejected papers, particularly from the ICLR dataset which includes both accepted and rejected papers, would provide valuable insights into the LLM&#x27;s ability to identify weaknesses and provide constructive criticism.</p>
        <div class="quote">"We began by examining the overlap between LLM feedback and human feedback on Nature family journal data (Supp. Table 1)." (Page 3)</div>
        <p><strong>Rationale:</strong> Analyzing feedback on rejected papers would shed light on the LLM&#x27;s ability to identify flaws, suggest improvements, and provide feedback that aligns with the reasons for rejection. This is crucial for assessing the LLM&#x27;s potential to help authors improve their manuscripts before submission.</p>
        <p><strong>Implementation:</strong> Conduct a separate analysis of LLM feedback on rejected papers from the ICLR dataset. Compare the overlap with human reviews, the specificity of the comments, and the focus on different aspects of scientific writing. This analysis could also explore whether LLM feedback aligns with the reasons for rejection provided by human reviewers.</p>
    </li>
    
            </ul>
            
            
    <h4>Non-Text Elements</h4>
    
    <details class="non-text-element">
        <summary>Figure 1</summary>
        <p>Figure 1 provides a visual overview of the study&#x27;s methodology, outlining the process of generating LLM scientific feedback using GPT-4 and the subsequent evaluation methods. It consists of three panels: (a) illustrates the pipeline for generating LLM feedback, starting with parsing a PDF and extracting relevant text to construct a prompt for GPT-4, which then generates structured feedback; (b) depicts the retrospective analysis, comparing LLM feedback to human feedback using a two-stage comment matching pipeline involving extractive text summarization and semantic text matching; (c) showcases the prospective user study, where researchers upload their papers and evaluate the LLM-generated feedback through a survey.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "Specifically, we developed a GPT-4 based scientific feedback generation pipeline that takes the raw PDF of a paper and produces structured feedback (Fig. 1a)."</p>
            <p><strong>Context:</strong> This sentence, appearing in the second paragraph of the Introduction section, introduces the study&#x27;s methodology and specifically mentions Figure 1a to illustrate the GPT-4 based feedback generation pipeline.</p>
        </div>
        
        <p><strong>Relevance:</strong> Figure 1 is crucial for understanding the study&#x27;s overall approach. It visually summarizes the key steps involved in generating LLM feedback, comparing it to human feedback, and gathering user perceptions, providing a clear roadmap for the subsequent sections of the paper.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The figure is well-organized, using distinct panels to separate the different stages of the methodology.</li><li>The use of arrows and color-coding effectively guides the reader through the flow of information.</li><li>The figure could benefit from a more detailed explanation of the comment matching pipeline in panel (b), perhaps using a separate sub-panel to illustrate the extraction and matching steps.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure accurately reflects the study&#x27;s methodology, providing a comprehensive overview of the key steps involved.</li><li>The figure highlights the two-pronged evaluation approach, emphasizing both the quantitative comparison to human feedback and the qualitative assessment through user surveys.</li><li>The figure could be strengthened by including a brief description of the specific criteria used for evaluating the quality of LLM feedback, such as accuracy, relevance, and helpfulness.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure 2</summary>
        <p>Figure 2 presents a retrospective analysis of LLM and human scientific feedback, comparing the overlap between comments raised by GPT-4 and human reviewers. It includes eight subfigures: (a) and (b) show the hit rate (percentage of overlapping comments) between GPT-4 and human reviewers for Nature Family Journals and ICLR, respectively, both with and without shuffling LLM feedback; (c) and (d) depict scatterplots comparing the overlap between LLM and human feedback across different Nature journals and ICLR decision outcomes, respectively; (e) and (f) illustrate the likelihood of GPT-4 identifying comments based on the number of human reviewers raising them (consensus count); (g) and (h) show the likelihood of GPT-4 identifying comments based on their position in the sequence of human feedback. The figure highlights that the overlap between LLM and human feedback is comparable to the overlap between two human reviewers, and that LLM is more likely to identify comments raised by multiple reviewers or appearing earlier in the feedback sequence.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "When comparing LLM feedback with comments from each individual reviewer, approximately one third (30.85%) of GPT-4 raised comments overlapped with comments from an individual reviewer (Fig. 2a)."</p>
            <p><strong>Context:</strong> This sentence, found in the third paragraph of the Results section under the subheading &#x27;LLM feedback significantly overlaps with human-generated feedback,&#x27; introduces the first key finding of the retrospective analysis and refers to Figure 2a to illustrate the overlap between LLM and human feedback.</p>
        </div>
        
        <p><strong>Relevance:</strong> Figure 2 is central to the Results section, providing strong evidence for the study&#x27;s main claim that LLM-generated feedback aligns with human feedback. It demonstrates the substantial overlap between GPT-4 and human reviewers, comparable to inter-reviewer agreement, and further explores factors influencing this overlap, such as reviewer consensus and comment position.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The figure is well-structured, using separate subfigures to present different aspects of the analysis.</li><li>The use of bar graphs, scatterplots, and error bars effectively conveys the data and its variability.</li><li>The figure could benefit from larger font sizes for axis labels and data point labels, especially in subfigures (c) and (d), to improve readability.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure provides a comprehensive analysis of the overlap between LLM and human feedback, using multiple metrics and exploring different factors influencing this overlap.</li><li>The shuffling experiment in subfigures (a) and (b) effectively addresses the potential concern of generic feedback, demonstrating that LLM feedback is paper-specific.</li><li>The figure could be strengthened by including statistical tests to assess the significance of the observed differences in overlap between different groups (e.g., GPT-4 vs. human, different journals, different decision outcomes).</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Hit rate (overlap) between GPT-4 and individual human reviewers for Nature Family Journals:</strong> 30.85 %</li><li><strong>Hit rate (overlap) between GPT-4 and individual human reviewers for ICLR:</strong> 39.23 %</li><li><strong>Hit rate (overlap) between two human reviewers for Nature Family Journals:</strong> 28.58 %</li><li><strong>Hit rate (overlap) between two human reviewers for ICLR:</strong> 35.25 %</li><li><strong>Decrease in hit rate after shuffling LLM feedback for Nature Family Journals:</strong> 30.42 %</li><li><strong>Decrease in hit rate after shuffling LLM feedback for ICLR:</strong> 35.32 %</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure 3</summary>
        <p>Figure 3 is a dot plot comparing the frequency of comments made by GPT-4 and human reviewers across 11 different aspects of scientific feedback, focusing on ICLR papers. The x-axis represents the log frequency ratio (GPT-4/Human), with positive values indicating that GPT-4 comments more frequently on that aspect and negative values indicating that humans comment more frequently. The y-axis lists the 11 aspects, including novelty, implications of research, add ablation experiments, add experiments on more datasets, etc. The size of each dot represents the prevalence of that aspect in human feedback. The figure highlights that LLM feedback emphasizes certain aspects more than humans, such as implications of research, while commenting less on others, like novelty. For example, GPT-4 comments on implications of research 7.27 times more frequently than humans, while being 10.69 times less likely to comment on novelty.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "Drawing on existing research in peer review literature within the machine learning domain41–44, we developed a schema comprising 11 distinct aspects of comments."</p>
            <p><strong>Context:</strong> This sentence, located in the fourth paragraph of the Results section under the subheading &#x27;LLM feedback emphasizes certain aspects more than humans,&#x27; introduces the analysis of comment aspects and precedes the mention of Figure 3.</p>
        </div>
        
        <p><strong>Relevance:</strong> Figure 3 provides further insights into the nature of LLM feedback, revealing differences in emphasis compared to human reviewers. It suggests that LLM and human feedback can complement each other, with each focusing on different aspects of scientific evaluation.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The dot plot effectively visualizes the differences in comment frequency between GPT-4 and human reviewers.</li><li>The use of a log scale for the frequency ratio is appropriate given the potential for large differences.</li><li>The figure could benefit from clearer labels for the x-axis, perhaps using a more descriptive term than &#x27;log frequency ratio&#x27; and indicating the direction of the ratio (GPT-4/Human).</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure provides a valuable analysis of the different aspects of scientific feedback emphasized by LLM and human reviewers.</li><li>The selection of 11 aspects based on existing literature and human annotation ensures the relevance and validity of the analysis.</li><li>The figure could be strengthened by including statistical tests to assess the significance of the observed differences in comment frequency between GPT-4 and human reviewers for each aspect.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Log frequency ratio (GPT-4/Human) for &#x27;Implications of Research&#x27;:</strong> 7.27</li><li><strong>Log frequency ratio (GPT-4/Human) for &#x27;Novelty&#x27;:</strong> -10.69</li><li><strong>Ratio of human comments requesting &#x27;Add Ablation Experiments&#x27; to LLM comments:</strong> 6.71</li><li><strong>Ratio of LLM comments requesting &#x27;Add Experiments on More Datasets&#x27; to human comments:</strong> 2.19</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure 4</summary>
        <p>Figure 4 presents the results of a prospective user study with 308 researchers, assessing their perceptions of LLM-generated feedback. It consists of eight horizontally-oriented bar charts (a-h), each representing a different aspect of LLM feedback: (a) overall helpfulness of LLM feedback; (b) alignment between LLM feedback and significant points/issues; (c) helpfulness of LLM feedback compared to human feedback; (d) specificity of LLM feedback compared to human feedback; (e) perceived improvement in review accuracy and thoroughness due to LLM feedback; (f) perceived decrease in workload for human feedback due to LLM feedback; (g) willingness to use the LLM feedback system again; (h) perceived benefit of LLM feedback for different roles (authors, reviewers, editors/area chairs). The figure highlights that researchers generally find LLM feedback helpful, with over 50% considering it helpful or very helpful, and that it aligns with human feedback to a significant extent. However, LLM feedback is perceived as slightly less helpful and specific compared to human feedback.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "The results from the user study are illustrated in Fig. 4."</p>
            <p><strong>Context:</strong> This sentence, appearing in the fifth paragraph of the Results section under the subheading &#x27;Prospective User Study and Survey,&#x27; introduces the findings of the user study and refers to Figure 4 for a visual representation of the data.</p>
        </div>
        
        <p><strong>Relevance:</strong> Figure 4 is crucial for understanding how researchers perceive LLM-generated feedback. It complements the retrospective analysis by providing subjective evaluations of the helpfulness, alignment, specificity, and potential benefits of LLM feedback, offering valuable insights into user acceptance and potential adoption.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The use of horizontally-oriented bar charts effectively presents the distribution of responses for each aspect of LLM feedback.</li><li>The consistent color scheme and clear labels for each response category facilitate easy interpretation.</li><li>The figure could benefit from directly labeling the numerical values on the x-axis (percentage scale) for improved readability.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure provides a comprehensive overview of user perceptions of LLM feedback, covering a range of relevant aspects.</li><li>The inclusion of comparisons to human feedback in subfigures (c) and (d) offers a valuable benchmark for assessing the LLM&#x27;s performance.</li><li>The figure could be strengthened by including statistical tests to assess the significance of the observed differences in perceptions between different response categories (e.g., helpful vs. unhelpful, more specific vs. less specific).</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Percentage of researchers finding LLM feedback &#x27;Helpful&#x27;:</strong> 50.3 %</li><li><strong>Percentage of researchers finding LLM feedback &#x27;Very Helpful&#x27;:</strong> 7.1 %</li><li><strong>Percentage of researchers finding LLM feedback &#x27;More Helpful&#x27; than human feedback:</strong> 20.4 %</li><li><strong>Percentage of researchers finding LLM feedback &#x27;About the Same&#x27; helpfulness as human feedback:</strong> 20.1 %</li><li><strong>Percentage of researchers finding LLM feedback &#x27;Less Helpful&#x27; than human feedback:</strong> 41.9 %</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Table Supplementary Table 1</summary>
        <p>Supplementary Table 1 summarizes the number of papers and associated reviews sampled from 15 Nature family journals used in the retrospective analysis. It lists the journals in the first column and provides the corresponding counts of papers and reviews in the second and third columns, respectively. The table shows that the dataset includes a total of 3,096 papers and 8,745 reviews, covering a range of journals within the Nature portfolio.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "The first dataset, sourced from Nature family journals, includes 8,745 comments from human reviewers for 3,096 accepted papers across 15 Nature family journals, including Nature, Nature Biomedical Engineering, Nature Human Behaviour, and Nature Communications (Supp. Table 1, Methods)."</p>
            <p><strong>Context:</strong> This sentence, appearing in the second paragraph of the Results section under the subheading &#x27;Retrospective Evaluation,&#x27; introduces the first dataset used for the retrospective analysis and refers to Supplementary Table 1 for a summary of the papers and reviews included.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Table 1 provides essential details about the scope and composition of the Nature family journals dataset, allowing readers to assess the representativeness and generalizability of the retrospective analysis. It demonstrates the large scale of the dataset, covering a diverse range of journals and a substantial number of papers and reviews.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The table is clear and well-organized, using a simple structure to present the data effectively.</li><li>The use of boldface for the total counts enhances readability.</li><li>The table could benefit from a brief explanation of the criteria used for selecting the journals and papers included in the dataset.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The table accurately reflects the composition of the Nature family journals dataset, providing the necessary information for understanding its scope.</li><li>The table could be strengthened by including additional information about the journals, such as their impact factors or subject areas, to provide a more comprehensive overview of the dataset&#x27;s characteristics.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Total number of papers in the Nature family journals dataset:</strong> 3096</li><li><strong>Total number of reviews in the Nature family journals dataset:</strong> 8745</li><li><strong>Number of papers from &#x27;Nature&#x27;:</strong> 773</li><li><strong>Number of reviews from &#x27;Nature&#x27;:</strong> 2324</li><li><strong>Number of papers from &#x27;Nature Communications&#x27;:</strong> 810</li><li><strong>Number of reviews from &#x27;Nature Communications&#x27;:</strong> 2250</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Table Supplementary Table 2</summary>
        <p>Supplementary Table 2 summarizes the ICLR papers and their associated reviews used in the retrospective analysis, grouped by decision outcome. It presents the data for two years, 2022 and 2023, showing the number of papers and reviews for each decision category: Accept (Oral), Accept (Spotlight), Accept (Poster), Reject after author rebuttal, and Withdrawn after reviews. The table indicates that the dataset includes a total of 1,709 papers and 6,506 reviews, covering both accepted and rejected papers from the ICLR conference.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "The second dataset comprises 6,505 comments from human reviewers for 1,709 papers from the International Conference on Learning Representations (ICLR), a leading venue for artificial intelligence research in computer science (Supp. Table 2, Methods)."</p>
            <p><strong>Context:</strong> This sentence, following the introduction of the Nature family journals dataset, introduces the second dataset used for the retrospective analysis and refers to Supplementary Table 2 for a summary of the ICLR papers and reviews included.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Table 2 provides crucial information about the ICLR dataset, complementing the Nature family journals dataset and allowing readers to assess the diversity and representativeness of the papers used in the retrospective analysis. It highlights the inclusion of both accepted and rejected papers, enabling a more nuanced evaluation of LLM feedback across different levels of paper quality.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The table is well-structured, using clear headings and row/column labels to organize the data effectively.</li><li>The separation of data for 2022 and 2023 facilitates comparisons across years.</li><li>The table could benefit from a brief explanation of the criteria used for selecting the papers and the decision categories.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The table accurately reflects the composition of the ICLR dataset, providing the necessary information for understanding its scope and the distribution of papers across decision categories.</li><li>The table could be strengthened by including additional information about the ICLR conference, such as its acceptance rate or the number of submissions, to provide a more comprehensive context for the data.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Total number of papers in the ICLR dataset:</strong> 1709</li><li><strong>Total number of reviews in the ICLR dataset:</strong> 6506</li><li><strong>Number of accepted papers (Oral) in 2022:</strong> 55</li><li><strong>Number of reviews for accepted papers (Oral) in 2022:</strong> 200</li><li><strong>Number of rejected papers in 2023:</strong> 212</li><li><strong>Number of reviews for rejected papers in 2023:</strong> 799</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Table Supplementary Table 3</summary>
        <p>Supplementary Table 3 presents the results of human verification conducted on the retrospective comment extraction and matching pipeline. It is divided into two subtables: (a) focuses on the extractive summarization stage, showing the counts of true positives (TP), false negatives (FN), and false positives (FP) for extracted comments, along with calculated precision, recall, and F1 score; (b) focuses on the semantic matching stage, providing a similar breakdown for matched and unmatched comment pairs according to both human and predicted matching, followed by precision, recall, and F1 score for the matching results. The table demonstrates the high accuracy of both stages of the pipeline, with an F1 score of 0.968 for extraction and 0.824 for matching.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "We validated the pipeline’s accuracy through human verification, yielding an F1 score of 96.8% for extraction (Supp. Table 3a, Methods) and 82.4% for matching (Supp. Table 3b, Methods)."</p>
            <p><strong>Context:</strong> This sentence, located in the third paragraph of the Results section under the subheading &#x27;Retrospective Evaluation,&#x27; describes the human verification process used to validate the comment matching pipeline and refers to Supplementary Table 3a and 3b for the detailed results.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Table 3 is essential for establishing the reliability of the comment matching pipeline, a crucial component of the retrospective analysis. The high accuracy demonstrated by the human verification process strengthens the validity of the overlap analysis and the subsequent conclusions drawn from it.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The use of subtables effectively separates the results for the two stages of the pipeline, enhancing clarity.</li><li>The table is well-organized, using clear headings and row/column labels to present the data.</li><li>The table could benefit from a brief explanation of the human verification process, including the number of annotators and the criteria used for assessing matches and non-matches.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The table provides a comprehensive overview of the human verification results, including counts, proportions, and F1 scores, allowing readers to assess the accuracy of both stages of the pipeline.</li><li>The table could be strengthened by including statistical tests to assess the significance of the observed differences between human and predicted matching, further supporting the claim of high accuracy.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>F1 score for extractive summarization:</strong> 0.968</li><li><strong>Precision for extractive summarization:</strong> 0.977</li><li><strong>Recall for extractive summarization:</strong> 0.96</li><li><strong>F1 score for semantic matching:</strong> 0.824</li><li><strong>Precision for semantic matching:</strong> 0.777</li><li><strong>Recall for semantic matching:</strong> 0.878</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 1</summary>
        <p>Supplementary Figure 1 presents two bar graphs (a and b) showing the fraction of GPT-4 comments that overlap with comments raised by at least one human reviewer for Nature family journals and ICLR, respectively. Both graphs compare the overlap for GPT-4 vs. all human reviewers and GPT-4 (shuffle) vs. all human reviewers, where shuffling refers to randomly pairing LLM feedback with human feedback from a different paper. The figure highlights that a significant portion of GPT-4 comments overlap with human comments, and that this overlap is substantially reduced after shuffling, indicating that LLM feedback is paper-specific.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "More than half (57.55%) of the comments raised by GPT-4 were raised by at least one human reviewer (Supp. Fig. 1a)."</p>
            <p><strong>Context:</strong> This sentence, appearing in the third paragraph of the Results section under the subheading &#x27;LLM feedback significantly overlaps with human-generated feedback,&#x27; presents the first key finding of the overlap analysis for Nature family journals and refers to Supplementary Figure 1a to illustrate the overlap between GPT-4 and human feedback.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 1 provides additional evidence for the study&#x27;s main claim that LLM-generated feedback aligns with human feedback. It visually demonstrates the substantial overlap between GPT-4 and human reviewers, further supporting the findings presented in Figure 2 and reinforcing the conclusion that LLM feedback is not merely generic.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The bar graphs effectively convey the key message, clearly showing the difference in overlap between GPT-4 vs. human and GPT-4 (shuffle) vs. human.</li><li>The use of error bars representing 95% confidence intervals enhances the visualization of data variability.</li><li>The figure could benefit from clearer labels for the x-axis, perhaps using a more descriptive term than &#x27;GPT-4 (shuffle)&#x27; and explicitly stating the meaning of shuffling.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure provides a clear and concise visualization of the overlap analysis, supporting the conclusion that LLM feedback is paper-specific.</li><li>The inclusion of the shuffling experiment effectively addresses the potential concern of generic feedback.</li><li>The figure could be strengthened by including statistical tests to assess the significance of the observed differences in overlap between GPT-4 vs. human and GPT-4 (shuffle) vs. human.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Percentage of GPT-4 comments overlapping with at least one human reviewer for Nature family journals:</strong> 57.55 %</li><li><strong>Percentage of GPT-4 comments overlapping with at least one human reviewer for ICLR:</strong> 77.18 %</li><li><strong>Percentage of shuffled GPT-4 comments overlapping with at least one human reviewer for Nature family journals:</strong> 1.13 %</li><li><strong>Percentage of shuffled GPT-4 comments overlapping with at least one human reviewer for ICLR:</strong> 3.91 %</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 2</summary>
        <p>Supplementary Figure 2 presents a robustness check on the retrospective evaluation using four alternative set overlap metrics: hit rate, Szymkiewicz-Simpson overlap coefficient, Jaccard index, and Sørensen-Dice coefficient. It consists of eight bar graphs (a-h), each comparing the overlap between GPT-4 vs. Human, Human vs. Human, Human (w/o control) vs. Human, and GPT-4 (shuffle) vs. Human for either Nature journals or ICLR. The figure demonstrates that the overlap between GPT-4 and human feedback is comparable to the overlap between two human reviewers across all four metrics, suggesting the robustness of the findings across different overlap measures.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "To examine the robustness of the results across different set overlap metrics, we also evaluated three additional metrics: the Szymkiewicz–Simpson overlap coefficient, the Jaccard index, and the Sørensen–Dice coefficient."</p>
            <p><strong>Context:</strong> This sentence, appearing in the fourth paragraph of the Methods section under the subheading &#x27;Overlap Metrics for Retrospective Evaluations and Control,&#x27; introduces the robustness check using alternative overlap metrics and precedes the mention of Supplementary Figure 2.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 2 strengthens the study&#x27;s findings by demonstrating that the observed overlap between LLM and human feedback is not dependent on a specific overlap metric. The consistent results across four different metrics enhance the reliability and generalizability of the conclusions drawn from the retrospective analysis.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The use of separate bar graphs for each metric and dataset (Nature journals vs. ICLR) facilitates clear comparisons.</li><li>The consistent color scheme and clear labels for each comparison group enhance readability.</li><li>The figure could benefit from a more concise presentation, perhaps combining the bar graphs for different metrics into a single panel with multiple subplots.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure effectively demonstrates the robustness of the overlap analysis, showing consistent results across different overlap metrics.</li><li>The inclusion of the &#x27;Human (w/o control)&#x27; comparison group provides a valuable benchmark for assessing the impact of controlling for the number of comments.</li><li>The figure could be strengthened by including statistical tests to assess the significance of the observed differences in overlap between different groups (e.g., GPT-4 vs. human, different metrics) for each dataset.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 3</summary>
        <p>Supplementary Figure 3 is a bar chart showing the perceived helpfulness of LLM-based scientific feedback among participants with varying levels of publishing experience. The x-axis represents different categories of publishing experience, ranging from &#x27;No experience&#x27; to &#x27;More than 10 years.&#x27; The y-axis represents the proportion of respondents. Each bar is divided into segments representing the proportion of respondents who rated the helpfulness of the feedback as &#x27;1. Highly unhelpful,&#x27; &#x27;2. Unhelpful,&#x27; &#x27;3. Neither unhelpful nor helpful,&#x27; &#x27;4. Helpful,&#x27; or &#x27;5. Highly helpful.&#x27; The figure demonstrates that LLM-based feedback is considered helpful across all levels of publishing experience, with a majority of respondents in each category rating it as &#x27;Helpful&#x27; or &#x27;Highly helpful.&#x27;</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "Individuals from different educational backgrounds, ranging from undergraduate to postgraduate levels, found the feedback equally helpful and aligned with human feedback. Similarly, whether an experienced or a novice researcher, participants across the spectrum of publishing and reviewing experience reported similar levels of satisfaction and utility from the LLM based feedback, indicating that LLM based feedback tools could potentially be helpful to a diverse range of population (Supp. Fig. 3,4)."</p>
            <p><strong>Context:</strong> This sentence, appearing in the fifth paragraph of the Results section under the subheading &#x27;Prospective User Study and Survey,&#x27; highlights the consistent perceptions of helpfulness and alignment across different demographic groups and refers to Supplementary Figures 3 and 4 to illustrate this point.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 3 supports the study&#x27;s claim that LLM-based feedback can be beneficial for a diverse range of researchers. It demonstrates that the perceived helpfulness of the feedback is not limited to specific experience levels, suggesting its potential applicability across the research community.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The bar chart effectively conveys the distribution of responses for each experience level, using clear labels and a consistent color scheme.</li><li>The horizontal orientation of the bars and the clear separation between categories enhance readability.</li><li>The figure could benefit from directly labeling the numerical values (proportions) on the y-axis for improved clarity.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure provides a valuable visualization of the user study data, demonstrating the consistent perceived helpfulness of LLM feedback across different experience levels.</li><li>The figure could be strengthened by including statistical tests to assess the significance of the observed differences in helpfulness ratings between different experience categories.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 4</summary>
        <p>Supplementary Figure 4 is a bar chart illustrating the perceived helpfulness of LLM-based scientific feedback among participants with different professional statuses. The x-axis represents various professional categories, including &#x27;Undergraduate Student,&#x27; &#x27;Master Student,&#x27; &#x27;Doctoral Student,&#x27; &#x27;Postdoc,&#x27; &#x27;Faculty or Academic Staff,&#x27; and &#x27;Researcher in Industry.&#x27; The y-axis represents the proportion of respondents. Each bar is segmented to show the proportion of respondents within each category who rated the helpfulness as &#x27;1. Highly unhelpful,&#x27; &#x27;2. Unhelpful,&#x27; &#x27;3. Neither unhelpful nor helpful,&#x27; &#x27;4. Helpful,&#x27; or &#x27;5. Highly helpful.&#x27; The figure demonstrates that LLM-based feedback is considered helpful across different professional statuses, with a majority of respondents in each category rating it as &#x27;Helpful&#x27; or &#x27;Highly helpful.&#x27;</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "Individuals from different educational backgrounds, ranging from undergraduate to postgraduate levels, found the feedback equally helpful and aligned with human feedback. Similarly, whether an experienced or a novice researcher, participants across the spectrum of publishing and reviewing experience reported similar levels of satisfaction and utility from the LLM based feedback, indicating that LLM based feedback tools could potentially be helpful to a diverse range of population (Supp. Fig. 3,4)."</p>
            <p><strong>Context:</strong> This sentence, appearing in the fifth paragraph of the Results section under the subheading &#x27;Prospective User Study and Survey,&#x27; highlights the consistent perceptions of helpfulness and alignment across different demographic groups and refers to Supplementary Figures 3 and 4 to illustrate this point.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 4 further supports the study&#x27;s argument that LLM-based feedback can be valuable for a wide range of researchers. It shows that the perceived helpfulness of the feedback is not restricted to specific professional roles, suggesting its potential utility for students, faculty, and researchers in industry alike.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The bar chart effectively presents the distribution of responses for each professional status, using clear labels and a consistent color scheme.</li><li>The horizontal orientation of the bars and the clear separation between categories enhance readability.</li><li>The figure could benefit from directly labeling the numerical values (proportions) on the y-axis for improved clarity.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure provides a valuable visualization of the user study data, demonstrating the consistent perceived helpfulness of LLM feedback across different professional statuses.</li><li>The figure could be strengthened by including statistical tests to assess the significance of the observed differences in helpfulness ratings between different professional categories.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    
        </div>
        
        <div id="section-3" class="section">
            <h3>Discussion</h3>
            
            <h4>Overview</h4>
            <p>The Discussion section reflects on the study&#x27;s findings regarding the potential of LLMs, specifically GPT-4, for providing scientific feedback. It acknowledges the substantial overlap between LLM-generated feedback and human peer reviews, as well as the positive user perceptions regarding the usefulness of such feedback. However, it also emphasizes the limitations of LLMs, particularly their bias towards certain aspects of feedback and their inability to fully replace expert human review. The section concludes by discussing potential future directions for research, including exploring the use of LLMs for error detection and correction and expanding the scope of evaluation to include non-English manuscripts.</p>
            
            <h4>Key Aspects</h4>
            <ul><li><strong>Overlap and User Perceptions:</strong>The section highlights the significant overlap observed between LLM-generated feedback and human peer reviews, both in terms of content and user perceptions. It notes that a majority of participants in the user study found the LLM feedback helpful and aligned with human feedback.</li><li><strong>Potential Benefits:</strong>The discussion emphasizes the potential benefits of LLM feedback, particularly for researchers who lack access to timely and high-quality feedback mechanisms, such as those from underprivileged regions or those facing challenges in securing peer reviewers. It suggests that LLM feedback could serve as a valuable tool for self-checking and improving manuscripts before submission.</li><li><strong>Limitations of LLMs:</strong>The section acknowledges the limitations of LLMs in providing scientific feedback, particularly their tendency to focus on certain aspects (e.g., suggesting additional experiments on more datasets) and their inability to provide in-depth critiques of method design. It emphasizes that LLM feedback should not be seen as a replacement for expert human review.</li><li><strong>Importance of Human Review:</strong>The discussion reiterates the importance of expert human review as the cornerstone of rigorous scientific evaluation. It argues that while LLMs can be helpful, they cannot substitute for the specific and thoughtful feedback provided by domain experts.</li><li><strong>Future Directions:</strong>The section outlines potential future research directions, including exploring the use of LLMs for error detection and correction in scientific papers, expanding the scope of evaluation to include manuscripts written in languages other than English, and integrating visual LLMs or specialized modules to comprehend and critique visual elements in scientific literature.</li></ul>
            
            <h4>Strengths</h4>
            <ul>
            
    <li>
        <strong>Balanced Perspective</strong>
        <p>The section provides a balanced perspective on the potential and limitations of LLMs for scientific feedback. It acknowledges the promising findings while also highlighting the areas where LLMs fall short and emphasizing the continued importance of human review.</p>
        <div class="quote">"Despite the potential of LLMs in providing timely and helpful scientific feedback, it is important to note that expert human feedback will still be the cornerstone of rigorous scientific evaluation." (Page 7)</div>
    </li>
    
    <li>
        <strong>Contextualization of Findings</strong>
        <p>The section effectively contextualizes the study&#x27;s findings within the broader scientific landscape. It discusses the challenges faced by researchers in accessing timely and high-quality feedback, particularly those from underprivileged backgrounds, and positions LLM feedback as a potential solution to address these inequalities.</p>
        <div class="quote">"This could be especially helpful for researchers who lack access to timely quality feedback mechanisms, e.g., researchers from traditionally underprivileged regions who may not have resources to access conferences, or even peer review (their works are much more likely than those of “mainstream” researchers to get desk rejected by journals and thus seldom go through the peer review process14)." (Page 7)</div>
    </li>
    
    <li>
        <strong>Clear Future Directions</strong>
        <p>The section outlines clear and specific future research directions, building upon the study&#x27;s findings and identifying areas where further investigation is needed. The suggestions for exploring error detection and correction, expanding language coverage, and integrating visual LLMs provide a roadmap for advancing the field.</p>
        <div class="quote">"One direction for future work is to explore the extent to which the proposed approach can help identify and correct errors in scientific papers. This would involve artificially introducing various types of errors, including typos, mistakes in data analyses, and errors in mathematical equations." (Page 8)</div>
    </li>
    
            </ul>
            
            <h4>Suggestions for Improvement</h4>
            <ul>
            
    <li>
        <strong>Deeper Discussion of Ethical Implications</strong>
        <p>While the section mentions the potential misuse of LLMs, a more in-depth discussion of the ethical implications would strengthen the argument. This could include exploring potential biases in LLM-generated feedback, the impact on the integrity of the peer review process, and the responsibility of researchers in using LLMs ethically.</p>
        <div class="quote">"It is also important to note the potential misuse of LLM for scientific feedback." (Page 7)</div>
        <p><strong>Rationale:</strong> A more comprehensive discussion of ethical considerations would demonstrate the authors&#x27; awareness of the potential risks associated with LLMs and contribute to a more responsible and nuanced approach to their development and deployment in scientific evaluation.</p>
        <p><strong>Implementation:</strong> Add a paragraph or subsection specifically addressing the ethical implications of using LLMs for scientific feedback. This could include discussing potential biases in LLM-generated feedback, the risk of authors using LLMs to artificially inflate the quality of their work, and the need for guidelines and best practices for ethical LLM use in research evaluation. Referencing existing literature on AI ethics in research would further strengthen this discussion.</p>
    </li>
    
    <li>
        <strong>Elaborate on the Complementary Roles of LLMs and Humans</strong>
        <p>The section states that LLM and human feedback can complement each other but doesn&#x27;t fully explore how this could be achieved in practice. Providing concrete examples of how LLMs and humans could collaborate in the feedback process would enhance the practical implications of the study.</p>
        <div class="quote">"While comparable and even better than some reviewers, the current LLM feedback cannot substitute specific and thoughtful human feedback by domain experts." (Page 7)</div>
        <p><strong>Rationale:</strong> A more detailed discussion of the complementary roles of LLMs and humans would provide a more nuanced and practical perspective on the future of scientific feedback. It would move beyond simply stating that they can complement each other to outlining specific scenarios and strategies for effective collaboration.</p>
        <p><strong>Implementation:</strong> Add a paragraph or subsection exploring different ways in which LLMs and humans could collaborate in the feedback process. This could include examples such as: LLMs providing initial feedback on manuscripts before human review, humans focusing on high-level aspects while LLMs check for specific details or errors, or LLMs assisting reviewers in identifying relevant literature or suggesting alternative methodologies. Discussing the potential benefits and challenges of each approach would further enrich this discussion.</p>
    </li>
    
    <li>
        <strong>Address the Limitations of the User Study</strong>
        <p>The section acknowledges the self-selection bias in the user study but doesn&#x27;t fully address its implications. Discussing how this bias might have influenced the findings and suggesting strategies for mitigating it in future research would strengthen the study&#x27;s rigor.</p>
        <div class="quote">"Our user study is limited in coverage of participant population and suffer from a self-selection issue." (Page 8)</div>
        <p><strong>Rationale:</strong> Acknowledging and addressing the limitations of the user study is crucial for ensuring the transparency and validity of the research. It demonstrates the authors&#x27; critical awareness of potential biases and their commitment to methodological rigor.</p>
        <p><strong>Implementation:</strong> Add a sentence or two discussing the potential impact of self-selection bias on the user study findings. For example, acknowledge that participants who opted into the study might be more receptive to LLM feedback or have a higher level of familiarity with AI technologies. Suggest strategies for mitigating this bias in future research, such as using random sampling techniques or recruiting participants from a wider range of backgrounds and experience levels.</p>
    </li>
    
            </ul>
            
            
        </div>
        
        <div id="section-4" class="section">
            <h3>Methods</h3>
            
            <h4>Overview</h4>
            <p>The Methods section outlines the data sources and procedures used in the study. It describes the datasets from Nature family journals and the ICLR conference, detailing how the papers and reviews were collected. The section also explains the process of generating scientific feedback using GPT-4, including the prompt design and the token limitations. Finally, it outlines the retrospective comment matching pipeline used to compare LLM and human feedback, involving extractive text summarization and semantic text matching, and highlights the human verification process used to validate the pipeline&#x27;s accuracy.</p>
            
            <h4>Key Aspects</h4>
            <ul><li><strong>Nature Family Journals Dataset:</strong>The study used a dataset of 3,096 accepted papers and 8,745 reviews from 15 Nature family journals published between January 1, 2022, and June 17, 2023. The data was sourced directly from the Nature website.</li><li><strong>ICLR Dataset:</strong>The study also used a dataset of 1,709 papers and 6,506 reviews from the International Conference on Learning Representations (ICLR) from 2022 and 2023. The dataset included papers with different decision outcomes (accepted with oral/spotlight/poster presentations, rejected, and withdrawn), sampled using a stratified method. The data was retrieved using the OpenReview API.</li><li><strong>LLM Feedback Generation:</strong>The study used OpenAI&#x27;s GPT-4 to generate scientific feedback. The pipeline parsed the PDF of the paper, extracted the title, abstract, figure and table captions, and the first 6,500 tokens of the main text to construct a prompt. The prompt instructed GPT-4 to generate structured feedback in four sections: significance and novelty, potential reasons for acceptance, potential reasons for rejection, and suggestions for improvement.</li><li><strong>Retrospective Comment Matching Pipeline:</strong>The study developed a two-stage pipeline to compare LLM and human feedback. The first stage used extractive text summarization with GPT-4 to extract comments from both LLM and human feedback. The second stage used semantic text matching with GPT-4 to identify matching comments between the two sources, with a similarity rating mechanism to ensure high-quality matches.</li><li><strong>Human Verification:</strong>The accuracy of the comment matching pipeline was validated through human verification. Two co-authors assessed the extracted comments for accuracy, achieving an F1 score of 96.8%. Three co-authors independently assessed the matched comments, achieving an F1 score of 82.4%.</li></ul>
            
            <h4>Strengths</h4>
            <ul>
            
    <li>
        <strong>Detailed Data Description</strong>
        <p>The section provides a thorough description of the two datasets used in the study, including the number of papers and reviews, the time period covered, the journals included, and the decision outcomes for the ICLR dataset. This level of detail enhances the transparency and reproducibility of the research.</p>
        <div class="quote">"Our dataset comprises papers from 15 Nature family journals, published between January 1, 2022, and June 17, 2023. We sourced papers from 15 Nature family journals, focusing on those published between January 1, 2022, and June 17, 2023. Within this period, our dataset includes 773 accepted papers from Nature with 2,324 reviews, 810 sampled accepted papers from Nature Communications with 2,250 reviews, and many others. In total, our dataset includes 3,096 accepted papers and 8,745 reviews (Supp. Table 1). The data were sourced directly from the Nature website (https://nature.com/)." (Page 8)</div>
    </li>
    
    <li>
        <strong>Clear Explanation of LLM Pipeline</strong>
        <p>The section clearly explains the process of generating scientific feedback using GPT-4, including the steps involved in parsing the PDF, constructing the prompt, and generating the structured feedback. This transparency allows readers to understand how the LLM was used and the limitations imposed by token constraints.</p>
        <div class="quote">"We prototyped a pipeline to generate scientific feedback using OpenAI’s GPT-419 (Fig. 1a). The system’s input was the academic paper in PDF format, which was then parsed with the machine-learning-based ScienceBeam PDF parser53. Given the token constraint of GPT-4, which allows 8,192 tokens for combined input and output, the initial 6,500 tokens of the extracted title, abstract, figure and table captions, and main text were utilized to construct the prompt for GPT-4 (Supp. Fig. 5)." (Page 8)</div>
    </li>
    
    <li>
        <strong>Rigorous Validation Process</strong>
        <p>The section describes a rigorous human verification process to validate the accuracy of the comment matching pipeline. The involvement of multiple co-authors in the assessment and the high F1 scores achieved for both extraction and matching demonstrate the reliability of the pipeline.</p>
        <div class="quote">"We validated our retrospective comment matching pipeline using human verification. In the extractive text summarization stage, we randomly selected 639 pieces of scientific feedback, including 150 from the LLM and 489 from human contributors. Two co-authors assessed each feedback and its corresponding list of extracted comments, identifying true positives (correctly extracted comments), false negatives (missed relevant comments), and false positives (incorrectly extracted or split comments). This process resulted in an F1 score of 0.968, with a precision of 0.977 and a recall of 0.960 (Supp. Table 3a), demonstrating the accuracy of the extractive summarization stage." (Page 9)</div>
    </li>
    
            </ul>
            
            <h4>Suggestions for Improvement</h4>
            <ul>
            
    <li>
        <strong>Provide Prompt Examples</strong>
        <p>While the section describes the structure and content of the prompts used for GPT-4, it would be beneficial to include actual examples of the prompts. This would allow readers to fully understand how the LLM was instructed and the specific wording used to elicit the desired feedback.</p>
        <div class="quote">"Following the reviewer report instructions from machine learning conferences29–33 and Nature family journals27, 28, we provided specific instructions to generate four feedback sections: significance and novelty, potential reasons for acceptance, potential reasons for rejection, and suggestions for improvement (Supp. Fig. 12)." (Page 9)</div>
        <p><strong>Rationale:</strong> Providing prompt examples would enhance the transparency and reproducibility of the research. It would allow other researchers to replicate the study or adapt the methodology for their own research, fostering further exploration of LLM-based feedback generation.</p>
        <p><strong>Implementation:</strong> Include a supplementary figure or appendix containing the actual prompts used for GPT-4, both for the Nature family journals dataset and the ICLR dataset. This could include the full text of the prompts, highlighting the specific instructions and the placeholders for paper content.</p>
    </li>
    
    <li>
        <strong>Discuss Limitations of Token Constraints</strong>
        <p>The section mentions the token limitations of GPT-4 but doesn&#x27;t fully discuss their potential impact on the feedback generation process. Addressing how these constraints might have affected the comprehensiveness or depth of the LLM feedback would strengthen the analysis.</p>
        <div class="quote">"Given the token constraint of GPT-4, which allows 8,192 tokens for combined input and output, the initial 6,500 tokens of the extracted title, abstract, figure and table captions, and main text were utilized to construct the prompt for GPT-4 (Supp. Fig. 5)." (Page 8)</div>
        <p><strong>Rationale:</strong> Acknowledging the limitations imposed by token constraints is crucial for a balanced assessment of the LLM&#x27;s capabilities. It would highlight potential areas where the feedback might be incomplete or superficial due to the limited input size.</p>
        <p><strong>Implementation:</strong> Add a paragraph or subsection discussing the potential impact of token constraints on the LLM feedback. This could include exploring whether the 6,500 token limit was sufficient to capture the full complexity of the papers, whether certain sections (e.g., results, discussion) were more likely to be truncated, and how these limitations might have affected the quality and depth of the feedback. Suggesting strategies for mitigating these limitations in future research, such as using LLMs with larger token capacities or developing methods for selectively extracting the most relevant information from papers, would further enhance this discussion.</p>
    </li>
    
    <li>
        <strong>Clarify Criteria for Similarity Rating</strong>
        <p>The section mentions a similarity rating mechanism used in the semantic text matching stage but doesn&#x27;t fully explain the criteria for assigning these ratings. Providing a more detailed description of the rating scale and the factors considered when assessing similarity would enhance the transparency and reproducibility of the matching process.</p>
        <div class="quote">"Given that our preliminary experiments showed GPT-4’s matching to be lenient, we introduced a similarity rating mechanism. In addition to identifying corresponding pairs of matched comments, GPT-4 was also tasked with self-assessing match similarities on a scale from 5 to 10 (Supp. Fig. 14)." (Page 9)</div>
        <p><strong>Rationale:</strong> A clear explanation of the similarity rating criteria would allow readers to understand how matches were assessed and the level of agreement required for comments to be considered overlapping. It would also enable other researchers to replicate the matching process or adapt it for their own research, fostering further exploration of LLM-based feedback comparison.</p>
        <p><strong>Implementation:</strong> Include a table or a more detailed description of the similarity rating scale, explaining the meaning of each rating level (e.g., 5 - Somewhat Related, 10 - Identical) and the specific factors considered when assigning ratings. This could include examples of matched comments with different similarity ratings, illustrating the nuances of the rating process. Discussing the rationale for choosing a threshold of 7 for filtering out weakly-matched comments would further enhance this explanation.</p>
    </li>
    
            </ul>
            
            
    <h4>Non-Text Elements</h4>
    
    <details class="non-text-element">
        <summary>Table Supplementary Table 4</summary>
        <p>Supplementary Table 4 presents the mean token lengths of both the papers and the human reviews for the two datasets used in the study: ICLR and Nature Family Journals. The table shows that ICLR papers have a mean token length of 5,841.46, while Nature Family Journal papers have a mean token length of 12,444.06. The mean token length for human reviews of ICLR papers is 671.53, and for Nature Family Journal papers, it is 1,337.93.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "This token limit exceeds the 5,841.46-token average of ICLR papers and covers over half of the 12,444.06-token average for Nature family journal papers (Supp. Table 4)."</p>
            <p><strong>Context:</strong> This sentence, found in the first paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Generating Scientific Feedbacks using LLM,&#x27; explains the rationale for using the initial 6,500 tokens of the extracted text to construct the prompt for GPT-4. It refers to Supplementary Table 4 to provide the average token lengths of ICLR and Nature Family Journal papers.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Table 4 is relevant to the &#x27;Methods&#x27; section as it provides context for the decision to use the initial 6,500 tokens of the extracted text for the GPT-4 prompt. It demonstrates that this token limit is sufficient to cover the majority of the content in both ICLR and Nature Family Journal papers, ensuring that the LLM has access to a substantial portion of the paper&#x27;s information when generating feedback.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The table is clear and concise, presenting the data in a straightforward manner.</li><li>The use of clear headings and labels makes the information easy to understand.</li><li>The table could benefit from a brief explanation of what &#x27;tokens&#x27; represent in this context, especially for readers unfamiliar with natural language processing.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The table provides valuable information about the average length of papers and reviews in the two datasets, which is relevant for understanding the scope of the study and the challenges of processing long documents with LLMs.</li><li>The table could be strengthened by including additional statistics, such as the standard deviation or range of token lengths, to provide a more complete picture of the data distribution.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Mean token length of ICLR papers:</strong> 5841.46 tokens</li><li><strong>Mean token length of Nature Family Journal papers:</strong> 12444.06 tokens</li><li><strong>Mean token length of human reviews for ICLR papers:</strong> 671.53 tokens</li><li><strong>Mean token length of human reviews for Nature Family Journal papers:</strong> 1337.93 tokens</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Table Supplementary Table 5</summary>
        <p>Supplementary Table 5 provides examples of comments extracted from both LLM (GPT-4) and human feedback on ICLR papers, categorized by human coding. The table includes comments related to &#x27;Clarity and Presentation,&#x27; &#x27;Comparison to Previous Studies,&#x27; &#x27;Theoretical Soundness,&#x27; &#x27;Novelty,&#x27; and &#x27;Reproducibility.&#x27; Each row presents a comment from either a human reviewer or GPT-4, highlighting different aspects of the paper&#x27;s strengths and weaknesses.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "Each comment was then annotated according to our predefined schema, identifying any of the 11 aspects it represented (Supp. Table 5,6,7)."</p>
            <p><strong>Context:</strong> This sentence, located in the last paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Characterizing the comment aspects in human and LLM feedback,&#x27; describes the human annotation process used to categorize comments based on the 11 key aspects. It refers to Supplementary Tables 5, 6, and 7 as examples of the annotated comments.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Table 5 is relevant to the &#x27;Methods&#x27; section as it provides concrete examples of the human annotation process used to categorize comments based on the 11 key aspects. It illustrates the types of comments that fall under each category, allowing readers to understand how the annotation schema was applied and the diversity of feedback captured in the analysis.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The table is well-organized, using clear headings and row labels to distinguish between human and LLM comments and the different coding categories.</li><li>The use of different fonts or colors to differentiate between human and LLM comments could further enhance readability.</li><li>The table could benefit from a brief explanation of the criteria used for selecting the example comments and the level of agreement between annotators.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The table provides valuable insights into the types of comments generated by both human reviewers and GPT-4, highlighting similarities and differences in their focus and level of detail.</li><li>The table could be strengthened by including a quantitative analysis of the prevalence of each coding category in both human and LLM feedback, providing a more comprehensive picture of the data.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Table Supplementary Table 6</summary>
        <p>Supplementary Table 6, similar to Table 5, presents examples of comments extracted from LLM (GPT-4) and human feedback on ICLR papers, categorized by human coding. This table focuses on comments related to &#x27;Add ablations experiments,&#x27; &#x27;Implications of the Research,&#x27; and &#x27;Ethical Aspects.&#x27; Each row provides a comment from either a human reviewer or GPT-4, showcasing their perspectives on these specific aspects of the papers.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "Each comment was then annotated according to our predefined schema, identifying any of the 11 aspects it represented (Supp. Table 5,6,7)."</p>
            <p><strong>Context:</strong> This sentence, located in the last paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Characterizing the comment aspects in human and LLM feedback,&#x27; describes the human annotation process used to categorize comments based on the 11 key aspects. It refers to Supplementary Tables 5, 6, and 7 as examples of the annotated comments.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Table 6 complements Table 5 by providing additional examples of the human annotation process, focusing on different coding categories. It further illustrates the diversity of feedback captured in the analysis and the ability of both human reviewers and GPT-4 to comment on various aspects of scientific research, including ethical considerations.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The table maintains the same clear structure as Table 5, using headings and row labels to organize the data effectively.</li><li>The use of different fonts or colors to distinguish between human and LLM comments could further enhance readability.</li><li>The table could benefit from a brief explanation of the criteria used for selecting the example comments and the level of agreement between annotators.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The table provides valuable insights into the types of comments generated by both human reviewers and GPT-4, particularly regarding the need for additional experiments, the broader implications of the research, and ethical considerations.</li><li>The table could be strengthened by including a quantitative analysis of the prevalence of each coding category in both human and LLM feedback, providing a more comprehensive picture of the data.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Table Supplementary Table 7</summary>
        <p>Supplementary Table 7 is identical to Supplementary Table 6, presenting the same examples of comments extracted from LLM (GPT-4) and human feedback on ICLR papers, categorized by human coding. The table includes comments related to &#x27;Add ablations experiments,&#x27; &#x27;Implications of the Research,&#x27; and &#x27;Ethical Aspects,&#x27; with each row providing a comment from either a human reviewer or GPT-4.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "Each comment was then annotated according to our predefined schema, identifying any of the 11 aspects it represented (Supp. Table 5,6,7)."</p>
            <p><strong>Context:</strong> This sentence, located in the last paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Characterizing the comment aspects in human and LLM feedback,&#x27; describes the human annotation process used to categorize comments based on the 11 key aspects. It refers to Supplementary Tables 5, 6, and 7 as examples of the annotated comments.</p>
        </div>
        
        <p><strong>Relevance:</strong> The repetition of Supplementary Table 6 as Supplementary Table 7 appears to be an error in the paper. It does not provide any additional information or insights beyond what is already presented in Table 6. Removing the duplicate table would improve the clarity and conciseness of the paper.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The table is identical to Table 6, using the same structure and content.</li><li>The repetition of the table is unnecessary and potentially confusing for readers.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The table does not provide any new information or insights beyond what is already presented in Table 6.</li><li>Removing the duplicate table would improve the paper&#x27;s clarity and conciseness.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 5</summary>
        <p>Supplementary Figure 5 provides a schematic representation of the LLM scientific feedback generation system. It illustrates the process of extracting manuscript text, including figure captions, from PDF files and integrating it into a prompt for GPT-4. The figure highlights that GPT-4 generates structured feedback in four sections: significance and novelty, potential reasons for acceptance, potential reasons for rejection, and suggestions for improvement. An example comment from GPT-4 is included, pointing out that a paper reported a modality gap phenomenon but did not propose methods to close the gap or demonstrate the benefits of doing so.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "We prototyped a pipeline to generate scientific feedback using OpenAI’s GPT-419 (Fig. 1a)."</p>
            <p><strong>Context:</strong> This sentence, located in the first paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Generating Scientific Feedbacks using LLM,&#x27; introduces the pipeline used to generate scientific feedback using GPT-4 and refers to Figure 1a for a visual representation. However, Figure 1a does not provide a detailed schematic of the system, which is instead presented in Supplementary Figure 5.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 5 is relevant to the &#x27;Methods&#x27; section as it provides a more detailed visual representation of the LLM scientific feedback generation system than Figure 1a. It clarifies the steps involved in extracting text from PDFs, constructing the prompt for GPT-4, and generating structured feedback. The inclusion of an example comment from GPT-4 further illustrates the system&#x27;s output and its ability to identify specific issues in scientific papers.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The figure is clear and easy to follow, using a simple flowchart to illustrate the process.</li><li>The use of different colors and shapes to represent different stages of the process enhances readability.</li><li>The figure could benefit from a more detailed explanation of the prompt construction process, perhaps using a separate sub-panel to show the specific elements included in the prompt.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure accurately reflects the steps involved in the LLM scientific feedback generation system, providing a comprehensive overview of the process.</li><li>The inclusion of an example comment from GPT-4 demonstrates the system&#x27;s ability to generate specific and potentially useful feedback.</li><li>The figure could be strengthened by including a brief explanation of the criteria used for evaluating the quality of GPT-4&#x27;s feedback, such as accuracy, relevance, and helpfulness.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 6</summary>
        <p>Supplementary Figure 6 illustrates the workflow of the retrospective comment matching pipeline used to compare comments raised in LLM-generated feedback with those from human reviewers. The pipeline consists of two stages: (a) Extraction, where key comments are extracted from both LLM-generated and human-written reviews using LLM&#x27;s information extraction capabilities; (b) Matching, where LLM is used for semantic similarity analysis to match comments from LLM and human feedback, providing a similarity rating and justifications for each paired comment. A similarity threshold of 7 or higher is used to filter out weakly-matched comments.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "We developed a retrospective comment matching pipeline to evaluate the overlap between feedback from LLM and human reviewers (Fig. 1b, Methods)."</p>
            <p><strong>Context:</strong> This sentence, located in the third paragraph of the &#x27;Results&#x27; section under the subheading &#x27;Retrospective Evaluation,&#x27; introduces the comment matching pipeline used to assess the overlap between LLM and human feedback. It refers to Figure 1b, which provides a high-level overview of the pipeline, but Supplementary Figure 6 offers a more detailed illustration of the workflow.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 6 is relevant to the &#x27;Methods&#x27; section as it provides a detailed visual representation of the comment matching pipeline, a crucial component of the retrospective analysis. It clarifies the two stages involved in the process: extraction of key comments and semantic matching based on similarity. The figure also highlights the use of a similarity threshold to ensure the quality of the matches.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The figure is clear and easy to understand, using a simple flowchart to illustrate the two stages of the pipeline.</li><li>The use of different colors and shapes to represent different elements of the process enhances readability.</li><li>The figure could benefit from a more detailed explanation of the semantic similarity analysis, perhaps using a separate sub-panel to show the criteria used for matching comments and the similarity rating scale.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure accurately reflects the steps involved in the retrospective comment matching pipeline, providing a comprehensive overview of the process.</li><li>The inclusion of the similarity threshold highlights the importance of ensuring the quality of the matches and avoiding false positives.</li><li>The figure could be strengthened by including a brief explanation of the rationale for choosing a threshold of 7 and the potential impact of different thresholds on the results.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Similarity threshold for filtering out weakly-matched comments:</strong> 7</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 8</summary>
        <p>Supplementary Figure 8 presents a robustness check on controlling for the number of comments when measuring overlap in feedback from Nature family journals. The figure is said to be stratified by nine specific journals: Nature, Nature Biomedical Engineering, Nature Cell Biology, Nature Ecology &amp; Evolution, Nature Human Behaviour, Nature Communications, Nature Immunology, Nature Microbiology, and Nature Structural &amp; Molecular Biology. The caption indicates that the figure would show results with and without controlling for the number of comments, suggesting a comparison between different approaches to measuring overlap. It also states that the overlap between LLM feedback and human feedback appears comparable to the overlap observed between two human reviewers. However, the figure itself is not visible in the provided image.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "The results, with and without this control, were largely similar across both the ICLR dataset for different decision outcomes (Supp. Fig. 7,10) and the Nature family journals dataset across different journals (Supp. Fig. 8,9,10)."</p>
            <p><strong>Context:</strong> This sentence, located in the fifth paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Overlap Metrics for Retrospective Evaluations and Control,&#x27; describes the robustness check performed to assess the impact of controlling for the number of comments when measuring overlap. It refers to Supplementary Figures 7, 8, 9, and 10 to illustrate the results of this check for both ICLR and Nature family journals datasets.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 8 is relevant to the &#x27;Methods&#x27; section as it is intended to demonstrate the robustness of the overlap analysis by comparing results with and without controlling for the number of comments. It aims to show that the observed overlap between LLM and human feedback is not significantly affected by this methodological choice, further supporting the study&#x27;s main findings. However, the absence of the figure itself limits the reader&#x27;s ability to assess the validity of this claim.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The figure is not visible in the provided image, making it impossible to evaluate its visual aspects.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The caption suggests that the figure would present a comparison between different approaches to measuring overlap, but the absence of the figure prevents a thorough analysis of the results.</li><li>The caption claims that the overlap between LLM and human feedback is comparable to inter-reviewer overlap, but this cannot be verified without seeing the actual data.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 9</summary>
        <p>Supplementary Figure 9 continues the robustness check on controlling for the number of comments when measuring hit rates in Nature family journal data, focusing on six additional journals: Communications Biology, Communications Chemistry, Communications Earth &amp; Environment, Communications Materials, Communications Medicine, and Communications Physics. The figure consists of six bar graphs, each representing a different journal. Each graph compares the hit rates (percentage of overlapping comments) for three scenarios: GPT-4 vs. Human, Human vs. Human, and GPT-4 (shuffle) vs. Human. The figure demonstrates that controlling for the number of comments does not significantly affect the results, and that the overlap between LLM feedback and human feedback is comparable to the overlap observed between two human reviewers.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "The results, with and without this control, were largely similar across both the ICLR dataset for different decision outcomes (Supp. Fig. 7,10) and the Nature family journals dataset across different journals (Supp. Fig. 8,9,10)."</p>
            <p><strong>Context:</strong> This sentence, located in the fifth paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Overlap Metrics for Retrospective Evaluations and Control,&#x27; describes the robustness check performed to assess the impact of controlling for the number of comments when measuring overlap. It refers to Supplementary Figures 7, 8, 9, and 10 to illustrate the results of this check for both ICLR and Nature family journals datasets.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 9 complements Figure 8 by providing further evidence for the robustness of the overlap analysis across different Nature family journals. It demonstrates that the observed overlap between LLM and human feedback is consistent across various journals and is not significantly affected by controlling for the number of comments, strengthening the study&#x27;s main findings.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The figure is well-organized, using separate bar graphs for each journal and a consistent color scheme to facilitate comparisons.</li><li>The use of error bars representing 95% confidence intervals enhances the visualization of data variability.</li><li>The figure could benefit from a more concise caption, perhaps moving some of the descriptive text to the main text of the paper.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure effectively demonstrates the robustness of the overlap analysis, showing consistent results across different journals and with/without controlling for the number of comments.</li><li>The inclusion of the &#x27;GPT-4 (shuffle)&#x27; comparison group provides a valuable benchmark for assessing the specificity of LLM feedback.</li><li>The figure could be strengthened by including statistical tests to assess the significance of the observed differences in hit rates between different groups (e.g., GPT-4 vs. human, different journals) for each scenario.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 10</summary>
        <p>Supplementary Figure 10 presents a robustness check on controlling for the number of comments in the correlation of hit rates. It consists of four scatter plots (a-d), each comparing the hit rates of GPT-4 vs. Human and Human vs. Human (with and without controlling for the number of comments) for either Nature family journals or ICLR papers with different decision outcomes. The figure demonstrates that the correlation between GPT-4 and human hit rates remains strong regardless of whether the number of comments is controlled for, suggesting that the findings are robust to this methodological choice.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "The results, with and without this control, were largely similar across both the ICLR dataset for different decision outcomes (Supp. Fig. 7,10) and the Nature family journals dataset across different journals (Supp. Fig. 8,9,10)."</p>
            <p><strong>Context:</strong> This sentence, located in the fifth paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Overlap Metrics for Retrospective Evaluations and Control,&#x27; describes the robustness check performed to assess the impact of controlling for the number of comments when measuring overlap. It refers to Supplementary Figures 7, 8, 9, and 10 to illustrate the results of this check for both ICLR and Nature family journals datasets.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 10 further supports the robustness of the overlap analysis by demonstrating that the correlation between GPT-4 and human hit rates is not significantly affected by controlling for the number of comments. This finding strengthens the study&#x27;s main claim that LLM-generated feedback aligns with human feedback, regardless of the specific method used to measure overlap.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The use of scatter plots effectively visualizes the correlation between GPT-4 and human hit rates.</li><li>The inclusion of a diagonal dashed line representing a 1:1 correlation facilitates interpretation.</li><li>The figure could benefit from clearer labels for the axes, perhaps using more descriptive terms than &#x27;GPT-4&#x27; and &#x27;Human&#x27; and indicating the specific metric being plotted (hit rate).</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The figure provides a clear and concise visualization of the correlation analysis, supporting the conclusion that the findings are robust to controlling for the number of comments.</li><li>The inclusion of both Nature family journals and ICLR datasets strengthens the generalizability of the findings.</li><li>The figure could be strengthened by including the correlation coefficients and p-values directly on the plots, rather than only in the caption.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Correlation coefficient (r) for hit rates across Nature family journals, controlling for the number of comments:</strong> 0.8</li><li><strong>P-value (P) for hit rates across Nature family journals, controlling for the number of comments:</strong> 0.000369</li><li><strong>Correlation coefficient (r) for hit rates across ICLR papers with different decision outcomes, controlling for the number of comments:</strong> 0.98</li><li><strong>P-value (P) for hit rates across ICLR papers with different decision outcomes, controlling for the number of comments:</strong> 0.00328</li><li><strong>Correlation coefficient (r) for hit rates across Nature family journals, without controlling for the number of comments:</strong> 0.75</li><li><strong>P-value (P) for hit rates across Nature family journals, without controlling for the number of comments:</strong> 0.00137</li><li><strong>Correlation coefficient (r) for hit rates across ICLR papers with different decision outcomes, without controlling for the number of comments:</strong> 0.98</li><li><strong>P-value (P) for hit rates across ICLR papers with different decision outcomes, without controlling for the number of comments:</strong> 0.00294</li></ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 11</summary>
        <p>Supplementary Figure 11 presents a screenshot of the web interface used for the prospective user study. The interface allows users to upload a research paper in PDF format and provide their email address to receive LLM-generated feedback. It includes instructions for uploading papers published after GPT-4&#x27;s last training cut-off in September 2021 and an ethics statement to discourage the direct application of LLM content for review-related tasks. The interface also displays a confirmation message indicating that the draft has been received and the review process will take approximately 5 minutes.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "To facilitate our user study, we launched an online Gradio demo55 of the aforementioned generation pipeline, accessible at a public URL (Supp. Fig. 11)."</p>
            <p><strong>Context:</strong> This sentence, located in the first paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Prospective User Study and Survey,&#x27; describes the online platform used for the user study and refers to Supplementary Figure 11 for a visual representation of the web interface.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 11 is relevant to the &#x27;Methods&#x27; section as it provides a visual representation of the user study platform, allowing readers to understand how participants interacted with the system and submitted their papers for LLM feedback. It also highlights the ethical considerations addressed in the study, such as discouraging the direct use of LLM content for review purposes.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The screenshot is clear and easy to understand, showing the key elements of the web interface.</li><li>The use of clear labels and instructions makes the interface user-friendly.</li><li>The ethics statement is partially cut off in the screenshot, which could be addressed by providing the full text in the caption or main text of the paper.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The interface effectively captures the necessary information for the user study, including the paper upload, email address, and ethical considerations.</li><li>The confirmation message provides clear feedback to the user, indicating that the submission has been received and the review process is underway.</li><li>The interface could be strengthened by including additional features, such as the ability to select specific aspects of feedback or provide additional context about the paper.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 12</summary>
        <p>Supplementary Figure 12 presents the prompt template employed with GPT-4 for generating scientific feedback on papers from the Nature journal family dataset. The template instructs GPT-4 to draft a high-quality review outline, starting with &#x27;Review outline:&#x27; and then providing four specific sections: &#x27;1. Significance and novelty,&#x27; &#x27;2. Potential reasons for acceptance,&#x27; &#x27;3. Potential reasons for rejection&#x27; (with at least two sub-bullet points for each key reason), and &#x27;4. Suggestions for improvement.&#x27; The template emphasizes the need for detailed and specific feedback, focusing on outlines only.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "This prompt is then fed into GPT-4, which generates the scientific feedback in a single pass. Further details and validations of the pipeline can be found in the Supplementary Information."</p>
            <p><strong>Context:</strong> This sentence, located in the first paragraph of the &#x27;Methods&#x27; section under the subheading &#x27;Generating Scientific Feedbacks using LLM,&#x27; describes the process of feeding the constructed prompt into GPT-4 to generate feedback. It refers to the Supplementary Information for further details, including the prompt template presented in Supplementary Figure 12.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 12 is relevant to the &#x27;Methods&#x27; section as it provides the specific instructions given to GPT-4 for generating scientific feedback. It clarifies the structure and content expected in the LLM-generated reviews, highlighting the focus on key aspects such as significance, novelty, acceptance/rejection reasons, and improvement suggestions. The template&#x27;s emphasis on detail and specificity aims to ensure the quality and usefulness of the feedback.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The prompt template is clear and well-organized, using headings and bullet points to structure the instructions effectively.</li><li>The use of boldface for key phrases emphasizes the importance of detail and specificity.</li><li>The template could benefit from a brief explanation of the rationale for choosing these specific sections and the level of detail expected in each section.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The prompt template effectively guides GPT-4 to generate structured and comprehensive feedback, covering key aspects of scientific evaluation.</li><li>The requirement for at least two sub-bullet points for each rejection reason encourages the LLM to provide detailed and specific justifications.</li><li>The template could be strengthened by including examples of high-quality feedback for each section, further guiding GPT-4 to generate more insightful and helpful reviews.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 13</summary>
        <p>Supplementary Figure 13 presents the prompt template used with GPT-4 for extractive text summarization of comments in both LLM and human feedback. The template instructs GPT-4 to identify key concerns raised in the review, focusing on potential reasons for rejection and ignoring minor issues like typos. The output is expected to be in JSON format, with each key representing a concern and the corresponding value containing a concise summary and the exact wording from the review. An example JSON format is provided to illustrate the expected structure.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "The pipeline first performs extractive text summarization34–37 to extract the comments from both LLM and human-written feedback."</p>
            <p><strong>Context:</strong> This sentence, located in the third paragraph of the &#x27;Results&#x27; section under the subheading &#x27;Retrospective Evaluation,&#x27; describes the first stage of the comment matching pipeline, which involves extracting comments from both LLM and human-written feedback using extractive text summarization. It refers to references 34-37 for further details on this technique, but Supplementary Figure 13 provides the specific prompt template used with GPT-4 for this purpose.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 13 is relevant to the &#x27;Methods&#x27; section as it provides the specific instructions given to GPT-4 for extracting key concerns from LLM and human feedback. It clarifies the focus on potential reasons for rejection and the expected JSON output format, ensuring consistency and facilitating the subsequent semantic matching stage of the pipeline.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The prompt template is clear and concise, using headings and bullet points to structure the instructions effectively.</li><li>The inclusion of an example JSON format provides a clear illustration of the expected output structure.</li><li>The template could benefit from a brief explanation of the rationale for focusing on potential reasons for rejection and ignoring minor issues.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The prompt template effectively guides GPT-4 to extract key concerns from the feedback, focusing on the most relevant aspects for evaluating paper quality.</li><li>The use of JSON format ensures a structured and machine-readable output, facilitating the subsequent semantic matching process.</li><li>The template could be strengthened by including examples of key concerns and their corresponding summaries, further guiding GPT-4 to extract the most relevant information.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        </ul></div>
    </details>
    
    <details class="non-text-element">
        <summary>Figure Supplementary Figure 14</summary>
        <p>Supplementary Figure 14 presents the prompt template employed with GPT-4 for semantic text matching to identify shared comments between two sets of feedback (LLM and human). The input consists of two lists of comments in JSON format, extracted using the prompt template shown in Supplementary Figure 13. GPT-4 is instructed to match points with a significant degree of similarity, avoiding superficial or weak connections. For each matched pair, GPT-4 is asked to provide a rationale for the match and rate the similarity on a scale of 5 to 10, with detailed descriptions of each similarity level.</p>
        
        <div class="first-mention">
            <h5>First Mention</h5>
            <p><strong>Text:</strong> "It then applies semantic text matching38–40 to identify shared comments between the two feedback sources."</p>
            <p><strong>Context:</strong> This sentence, following the description of extractive text summarization, describes the second stage of the comment matching pipeline, which involves using semantic text matching to identify shared comments between LLM and human feedback. It refers to references 38-40 for further details on this technique, but Supplementary Figure 14 provides the specific prompt template used with GPT-4 for this purpose.</p>
        </div>
        
        <p><strong>Relevance:</strong> Supplementary Figure 14 is relevant to the &#x27;Methods&#x27; section as it provides the specific instructions given to GPT-4 for matching comments based on semantic similarity. It clarifies the criteria for matching, the expected output format (JSON with rationale and similarity rating), and the detailed descriptions of each similarity level, ensuring consistency and facilitating the quantitative analysis of overlap.</p>
        
        <div class="critique-section">
            <div class="critique-title">Critique</div>
        
            <div class="critique-section">
                <div class="critique-title">Visual Aspects</div>
                <ul>
                <li>The prompt template is well-organized, using headings, bullet points, and a numbered scale to structure the instructions effectively.</li><li>The inclusion of detailed descriptions for each similarity level enhances clarity and reduces ambiguity in the matching process.</li><li>The template could benefit from a brief explanation of the rationale for choosing this specific similarity scale and the potential impact of different scales on the results.</li>
                </ul>
            </div>
            
            <div class="critique-section">
                <div class="critique-title">Analytical Aspects</div>
                <ul>
                <li>The prompt template effectively guides GPT-4 to perform semantic text matching, focusing on identifying comments with a significant degree of similarity and avoiding superficial matches.</li><li>The requirement for a rationale for each match encourages the LLM to provide justifications for its decisions, enhancing transparency and facilitating human validation.</li><li>The template could be strengthened by including examples of matched comments with their corresponding rationales and similarity ratings, further guiding GPT-4 to perform accurate and consistent matching.</li>
                </ul>
            </div>
            </div>
        
        <div class="numeric-data">
            <h5>Numeric Data</h5>
            <ul>
        <li><strong>Minimum similarity rating for a match:</strong> 5</li><li><strong>Maximum similarity rating for a match:</strong> 10</li></ul></div>
    </details>
    
    
        </div>
        
    </div>
    
    <a href="#" class="back-to-top">↑ Back to Top</a>
    
    <script>
        // Show/hide back-to-top button
        window.onscroll = function() {
            var backToTopButton = document.querySelector('.back-to-top');
            if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
                backToTopButton.style.display = 'block';
            } else {
                backToTopButton.style.display = 'none';
            }
        };
    </script>
</body>
</html>
    